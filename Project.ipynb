{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# laoding the first video (video containing the frame)\n",
    "cap = cv2.VideoCapture('frame2.mp4')\n",
    "# loading the second video\n",
    "val = cv2.VideoCapture('temp.mp4')\n",
    "# finding the width of the second video\n",
    "width = val.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "# finding the height of the second video\n",
    "height = val.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "# finding the width of the first video\n",
    "tvw = cap.get(cv2.CAP_PROP_FRAME_WIDTH )\n",
    "# finding the height of the first video\n",
    "tvh = cap.get(cv2.CAP_PROP_FRAME_HEIGHT )\n",
    "\n",
    "# Lucas kanade most used default params\n",
    "lk_params = dict(winSize = (15, 15),\n",
    "maxLevel = 4,\n",
    "criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# function to take the four corner points of the frame and creating a mask \n",
    "# which can be used to make the polygon region enclosed by the four corners black.\n",
    "def getMask(points):\n",
    "  # converting the numpy array so that dtype of the numpy array is int32\n",
    "\tpoints = points.astype('int32')\n",
    "  # initializing the mask size same as the size of the first video.\n",
    "\tmask = np.ones((int(tvh),int(tvw)), dtype=np.uint8)\n",
    "  # now using the filloply function we make the polygon region defined by the points numpy array black\n",
    "  # in the previously created mask.\n",
    "\tcv2.fillPoly(mask, pts = [points], color =(0,0,0))\n",
    "  # returning the mask created.\n",
    "\treturn mask\n",
    "\n",
    "# defining a list corners whcih contains the corners of the frame.\n",
    "corners = []\n",
    "cnt = 1\n",
    "ret, frame = cap.read()\n",
    "# function used to make user define the corners of the frame initially.\n",
    "def selectCorners(event, x, y, flags, params):\n",
    "    global cnt\n",
    "    # storing the corner position in the corners list when mouse left button is used.\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        point = (x,y)\n",
    "        corners.append(point) \n",
    "        cv2.putText(frame, str(len(corners)), (x, y), cv2.FONT_ITALIC, 1, (0, 0, 255), 2)\n",
    "    # after choosing all the 4 corners user should press right button so that the algorithm continues.\n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        cnt = cnt+1\n",
    "\n",
    "cv2.namedWindow(\"Frame\")\n",
    "# call back function so that the mouse clicks are treated as we expect (i.e as per the rules defined in the selectCorners function).\n",
    "cv2.setMouseCallback('Frame', selectCorners)\n",
    "\n",
    "# bellow lines of code make sure that the program waits till all the four corners of the \n",
    "# frame is selected by the user.\n",
    "while(cnt != 6):\n",
    "  cv2.imshow('Frame', frame)\n",
    "  if len(corners) == cnt:\n",
    "    cv2.circle(frame, corners[cnt-1], 5, (255, 0, 0), 2)\n",
    "    cnt = cnt + 1\n",
    "  cv2.waitKey(1)\n",
    "\n",
    "# initializing the variables oldframe and oldcorners which are used by the calcOpticalFlowPyrLK\n",
    "# funciton.\n",
    "oldframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "oldcorners = np.asarray(corners, dtype=np.float32)\n",
    "\n",
    "while(True):\n",
    "  # reading the frames (note: not the tv frame) from the first video   \n",
    "  ret, frame = cap.read()\n",
    "  # reading the frames (note: not the tv frame) from the second video   \n",
    "  ret2 , valframe = val.read()\n",
    "  # we stop if either first video or second video is finished\n",
    "  if ret == True and ret2 == True:\n",
    "    # converting the frame from the first video to gray scale.\n",
    "    grayImg = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # using the calcOpticalFlowPyrLK function which takes the old frame (note: not the tv frame) and the current \n",
    "    # frame (note: not the tv frame), and the old positions of the corners then the function calculates the new position of the corners. \n",
    "    newcorners, status, error = cv2.calcOpticalFlowPyrLK(oldframe, grayImg, oldcorners, None, **lk_params)\n",
    "    # update the oldframe variable.\n",
    "    oldframe = grayImg\n",
    "    # update the oldcorners variable. \n",
    "    oldcorners = newcorners\n",
    "    # pt1 defines the four corner points of the second video\n",
    "    pt1 = np.float32([[0,0],[width,0], [width,height], [0,height]])\n",
    "    # marking the newly found corners using the calcOpticalFlowPyrLK function\n",
    "    for corner in newcorners:\n",
    "      x, y = corner\n",
    "      # function used to mark the corners\n",
    "      cv2.circle(frame, (int(x),int(y)), 5, (0, 255, 0), -1)\n",
    "    # getting the perspective transform using the getPerspectiveTransform function.\n",
    "    # arguments passed are the four corners of the second video stored in the pt1 variable \n",
    "    # and the newcorners variable which contains the newly updated corners of the tv frame.\n",
    "    matrix = cv2.getPerspectiveTransform(pt1, newcorners)\n",
    "    # warpPerspective function is used to apply the previously found matrix on the second video.\n",
    "    # we also make sure that the size of the resultant image is same as the size of the first video.\n",
    "    result = cv2.warpPerspective(valframe, matrix, (int(tvw), int(tvh)))\n",
    "    # now we use the getMask function \n",
    "    mask = getMask(newcorners)\n",
    "    # once we get the mask we apply this mask for each frame (note: not the tv frame) of the \n",
    "    # first video (which is defined by the frame vairable) i.e we apply the mask in every iteration \n",
    "    # for the frame in the first video.\n",
    "    newtv = cv2.bitwise_and(frame, frame, mask = mask)\n",
    "    # we combine the newtv variable which contains the tv frame region blacked out in a frame (note: not the tv frame)\n",
    "    # of the first video and the result which is the perspective transformed version of a frame (note: not the tv frame) \n",
    "    # of the second video. this is nothing but the desired output which is shown using the imshow function of the cv2.\n",
    "    cv2.imshow(\"frame\", newtv+result)\n",
    "    # the below code is to ensure that we quit the output video when we press the key 'q' on the keyboard.\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "   \n",
    "  # the below code is to ensure that we exit when either first video or second video is finished.\n",
    "  else: \n",
    "    break\n",
    "\n",
    "# Closes all the frames once we reach end of the code.\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "626c03f8b9661600665115886653adc565855ff8475fcb5d08394c7d92ae7062"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
